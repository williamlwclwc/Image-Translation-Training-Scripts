Automatically translating between different representations of a scene given 
adequate training data 
is one of the popular applications of recent generative models. 
The work of Isola et al.\cite{pix2pix2016} has proved that 
Generative Adversarial Network(GAN) is an effective end-to-end way for generating images 
with rich details such as photorealistic images from sketches like the edge maps or segmentation maps. 
Following this work, more models have been proposed to improve the results. 

This report starts with a background introduction to the basics of these topics
and then discuss the principles behind the state-of-the-art models 
which translate segmentation maps into photorealistic images. 
Moreover, I manage to implement two recent high computational resources demanded 
state-of-the-art models with limited computational resources on a smaller dataset. 
The implementation details are shown and 
the results of the models are analyzed in this report.



